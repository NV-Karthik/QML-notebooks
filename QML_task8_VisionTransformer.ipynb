{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MTxcJ2a_bio4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-M005WqVbqN9",
    "outputId": "171be69f-7efe-4271-b969-e34244477e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 81865801.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 17871893.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 23860983.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 10738742.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: Prepare the MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "luVTPT1Tb0G8"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define the Vision Transformer Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_embedding = nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim), num_layers=depth)\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = torch.cat((self.positional_embedding.repeat(B, 1, 1), x), dim=1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heBJme9qb7p7",
    "outputId": "3ef8139f-6ac7-4c9b-9f40-591a17b3ba48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Training\n",
    "image_size = 28\n",
    "patch_size = 7\n",
    "num_classes = 10\n",
    "dim = 64\n",
    "depth = 6\n",
    "heads = 8\n",
    "mlp_dim = 128\n",
    "num_epochs = 10\n",
    "\n",
    "model = VisionTransformer(image_size, patch_size, num_classes, dim, depth, heads, mlp_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMkHw-rFb-gh",
    "outputId": "7eb80b76-0a61-40ba-fe35-ca628880386c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1184\n",
      "Epoch [2/10], Loss: 0.2773\n",
      "Epoch [3/10], Loss: 0.1810\n",
      "Epoch [4/10], Loss: 0.1451\n",
      "Epoch [5/10], Loss: 0.1245\n",
      "Epoch [6/10], Loss: 0.1134\n",
      "Epoch [7/10], Loss: 0.1041\n",
      "Epoch [8/10], Loss: 0.0963\n",
      "Epoch [9/10], Loss: 0.0903\n",
      "Epoch [10/10], Loss: 0.0831\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_RZEQK-EcEqk",
    "outputId": "6fd5ce9f-5004-4e9b-9d36-3ddba878d282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 97.54%\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYLrc9zWmvEV"
   },
   "source": [
    "# Comments on potential areas of extending Classical Vision Transformers to Quantum Vision Transformers\n",
    "\n",
    "1. **Conceptual Differences:**\n",
    "\n",
    "    1.1 **Input Representation**: In a CVT, the input is a preprocessed image represented as a 2D tensor. In a QVT, the input image would be encoded into a quantum state using a suitable encoding method (e.g., Amplitude Encoding or NEQR).  \n",
    "    1.2 **Self-Attention**: In a CVT, self-attention uses linear projections and softmax functions. In a QVT, we can explore these options:\n",
    "        Quantum Gates: Apply parameterized quantum gates (e.g., Ry, Rz) on qubits representing different image patches to learn relationships.\n",
    "   Quantum Entanglement: Utilize entanglement between qubits to capture long-range dependencies within the image.\n",
    "\n",
    "\n",
    "2. **Description of a possible QVT Architecture:**\n",
    "\n",
    "        2.1 Quantum Image Encoding: Encode the preprocessed image into a quantum state using a chosen encoding method. This creates a superposition representing the image features.\n",
    "   \n",
    "        2.2 Patch-wise Encoding: Divide the image into patches and encode each patch into a separate quantum state. Apply parameterized quantum circuits to each state to extract features.\n",
    "\n",
    "\n",
    "       2.3 Interacting and Entangled Qubits: Allow qubits representing different image patches to interact using controlled operations (CNOT) based on their relative positions or features. This can capture spatial or feature-based relationships.\n",
    "\n",
    "       2.4 Quantum Attention Function: Design a quantum circuit that acts as the attention function, potentially using entanglement and measurements to perform a weighted average of the encoded patches.\n",
    "\n",
    "       2.5 Apply parameterized quantum circuits (series of Ry, Rz, and controlled rotations) on the output of the self-attention layer to further extract high-level features.\n",
    "\n",
    "       2.6 Hybrid Networks: with softwares such as pennylane supporting qnn's where quantum layers can be sandwiched between classical layers also, we can think of replacing only specific parts of the vision transformer network. They can be applied where clear quantum advantages are expected, where phenomena like relative phases, phase kickbacks, entanglement of pixels of two regions etc., are known to happen\n",
    "\n",
    "       2.7 Hybrid training structure, even in case of QVT, like others, with Quantum Forward pass circuits and classical dataloading, optimization loops would be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
